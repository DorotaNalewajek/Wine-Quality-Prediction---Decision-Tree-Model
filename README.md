# ğŸ· Wine Quality Machine Learning

Mini machine-learning project that predicts **wine quality** based on its chemical properties.  
The repo contains **two classic ML models**:

- ğŸŒ³ **Decision Tree Classifier**
- ğŸ¤ **k-Nearest Neighbours (k-NN)**

## Goal: practise the full ML workflow on a real dataset â€“ from loading CSVs to evaluating and comparing models.

---

## ğŸ“ Project Structure

wine_quality_machine_learning/
â”œâ”€â”€ decision_tree/
â”‚   â””â”€â”€ wine_quality_decisiontree_load_from.py   # Decision Tree model
â”œâ”€â”€ knn/
â”‚   â””â”€â”€ winequality_knn_model.py                 # k-NN model
â”œâ”€â”€ wine_quality_data/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ wine_quality-red.csv
â”‚       â”œâ”€â”€ wine_quality-white.csv
â”‚       â””â”€â”€ wine_quality.names.csv
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


## ğŸ“Š Data Description

The project uses a public Wine Quality dataset (red & white wine).
Each row describes one wine sample with features such as:
	â€¢	fixed acidity, volatile acidity, citric acid
	â€¢	residual sugar, chlorides, free / total sulfur dioxide
	â€¢	density, pH, sulphates, alcohol
	â€¢	quality â€“ the target label (integer score, e.g. 3â€“8)

All CSV files live in wine_quality_data/data/.

----

## âš™ï¸ Requirements

- Python **3.10+**
- Recommended packages: `numpy`, `pandas`, `scikit-learn`, `matplotlib`

Install them (once) with:

```bash
pip install numpy pandas scikit-learn matplotlib
```

# â–¶ï¸ How to Run

## 1ï¸âƒ£ Decision Tree model (ğŸŒ³)

```bash
cd decision_tree
python wine_quality_decisiontree_load_from.py
```

##  - The script:
	1.	Loads data from ../wine_quality_data/data/.
	2.	Splits it into train / test sets.
	3.	Trains a DecisionTreeClassifier.
	4.	Prints accuracy and basic metrics.
	5.	Optionally plots the tree / feature importance.

â¸»

## 2ï¸âƒ£ k-Nearest Neighbours model (ğŸ¤)

```bash
cd knn
python winequality_knn_model.py
```

## ğŸ“ˆ Results â€“ High Level

The main focus is learning, not leaderboard scores â€“ models are only lightly tuned.


## In my experiments:
	â€¢	ğŸŒ³ Decision Tree: around ~80% test accuracy
	â€¢	ğŸ¤ k-NN: similar accuracy, depending on k and scaling

Exact numbers may vary between runs (random train/test split).

â¸»

## ğŸ¯ What I Practised Here
	â€¢	Working with real CSV data in Python.
	â€¢	Building a full ML pipeline with scikit-learn:
	â€¢	loading â†’ preprocessing â†’ training â†’ evaluation â†’ visualisation.
	â€¢	Comparing two classic algorithms:
	â€¢	Decision Tree vs k-Nearest Neighbours.
	â€¢	Organising a small ML repo for my portfolio:
	â€¢	clear folder structure for data and models,
	â€¢	readable, beginner-friendly code layout.

## ğŸš€ Possible Next Steps
	â€¢	Add hyperparameter search (GridSearchCV / RandomizedSearchCV).
	â€¢	Use cross-validation instead of a single train/test split.
	â€¢	Add more visualisations (confusion matrix, feature importance).
	â€¢	Try extra models (Random Forest, Gradient Boosting, etc.).
	â€¢	Wrap the best model into a small API or CLI tool.

â¸»

## ğŸ‘©â€ğŸ’» Author

Dorota Nalewajek â€“ future AI / ML developer & wine-quality detective ğŸ·ğŸ¤–



